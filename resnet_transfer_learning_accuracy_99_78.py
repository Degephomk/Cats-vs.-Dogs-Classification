# -*- coding: utf-8 -*-
"""resnet-transfer-learning-accuracy-99-78.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gTeIFQLvQnD3FAShyzCMZayb25dZnmkz
"""

from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.transforms import v2
from torchvision import models
from torchvision.models.resnet import ResNet, Bottleneck
import cv2
import os
import random
import matplotlib.pyplot as plt
from tqdm import tqdm

"""# Dataset and DataLoader"""

class CatsDogsDataset(Dataset):
    def __init__(self, path, train=True, transform=None):
        self.path = os.path.join(path, "train" if train else "test")
        self.images = os.listdir(self.path)
        random.shuffle(self.images)
        self.labels = [int(img[0]) for img in self.images]
        self.length = len(self.labels)
        self.target = torch.eye(2)
        self.transform = transform
        self.train = train

    def __getitem__(self, indx):
        image = cv2.imread(os.path.join(self.path, self.images[indx]))
        image = torch.from_numpy(image).permute(2, 0, 1) / 255
        label = self.labels[indx]
        target = self.target[label]

        if self.transform:
            image = self.transform(image)

        return image, target

    def __len__(self):
        return self.length

path = r"datasets\dogs_and_cats"

transforms = v2.Compose( [v2.Resize((360, 360)),
                          v2.RandomCrop((340, 340)),
                          v2.RandomAffine(degrees=(-10, 10),
                                          translate=(0.05, 0.05),
                                          scale=(0.95, 1.05),
                                          shear=(-3, 3),
                                          fill=(0)),
                          v2.RandomPhotometricDistort(),
                          v2.RandomHorizontalFlip()] )

train_dataset = CatsDogsDataset(path, train=True, transform=transforms)
test_dataset = CatsDogsDataset(path, train=False, transform=v2.Resize((340, 340)))

batch_size = 8

train_loader = DataLoader(train_dataset,
                          batch_size=batch_size,
                          shuffle=True,
                          drop_last=True)

test_loader = DataLoader(test_dataset,
                         batch_size=batch_size,
                         shuffle=False,
                         drop_last=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

device

"""# Techniques to be used:

- **Cosine learning rate decay**: the learning rate will decrease in cosine.

- **No bias decay**: Removing bias from regularisation (weight decay).

- **Stochastic depth**: Some of the layers will be randomly disabled as in Dropout.

- **Transfer learning**
"""

net = nn.Sequential(nn.Linear(10, 10))
optimizer_ = optim.Adam(net.parameters(), lr=0.1)
scheduler_ = optim.lr_scheduler.CosineAnnealingLR(optimizer_, T_max=20)

lr = []
for _ in range(20):
    lr.append(optimizer_.param_groups[0]["lr"])
    optimizer_.step()
    scheduler_.step()

plt.title("Cosine learning rate decay")
plt.xlabel("Epochs");
plt.ylabel("Learning rate");
plt.plot(lr);

"""# Bottleneck block using Stochastic depth"""

class BottleneckSD(Bottleneck):
    def __init__(self, *args, drop_prob=0.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.drop_prob = drop_prob

    def forward(self, x):
        identity = x

        if self.downsample is not None:
            identity = self.downsample(x)

        if self.training and torch.rand(1).item() < self.drop_prob:
            return identity

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.training:
            out = out / (1 - self.drop_prob)

        out += identity
        out = self.relu(out)
        return out

"""# Resnet152 with new Bottleneck blocks"""

class ResNet152SD(ResNet):
    def __init__(self, drop_prob=0, **kwargs):
        super().__init__(BottleneckSD, [3, 8, 36, 3], **kwargs)
        total_blocks = sum([8, 36, 3])
        block_id = 1
        for layer in [self.layer2, self.layer3, self.layer4]:
            for i in range(len(layer)):
                current_drop_prob = drop_prob * float(block_id) / total_blocks
                layer[i].drop_prob = current_drop_prob
                block_id += 1

"""# Loading the weights of the pre-trained model"""

pretrained_model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)
model = ResNet152SD(drop_prob=0.15)
model.load_state_dict(pretrained_model.state_dict())

epochs = 5

"""# Freezing a part of the layers"""

model.requires_grad_(False);

for layer in [model.layer2, model.layer3, model.layer4, model.fc]:
    layer.requires_grad_(True)

model.fc = nn.Linear(2048, 2).requires_grad_(True)
model = model.to(device)

"""# No bias decay"""

decay = dict()
no_decay = dict()
for name, param in model.named_parameters():
    if 'weight' in name:
        decay[name] = param
    else:
        no_decay[name] = param

layer2_weights = []
layer3_weights = []
layer4_weights = []
fc_weights = []
for name, weight in decay.items():
    if "layer2" in name:
        layer2_weights.append(weight)
    elif "layer3" in name:
        layer3_weights.append(weight)
    elif "layer4" in name:
        layer4_weights.append(weight)
    elif "fc" in name:
        fc_weights.append(weight)
    else:
        print(f"{name} is not included!")

layer2_bias = []
layer3_bias = []
layer4_bias = []
fc_bias = []
for name, weight in no_decay.items():
    if "layer2" in name:
        layer2_bias.append(weight)
    elif "layer3" in name:
        layer3_bias.append(weight)
    elif "layer4" in name:
        layer4_bias.append(weight)
    elif "fc" in name:
        fc_bias.append(weight)
    else:
        print(f"{name} is not included!")

"""# Each layer will have its own learning rate. The learning rate will be lower for earlier layers, and higher for later layers. We remove regularisation for bias."""

optimizer = optim.AdamW([
    {"params": layer2_weights, "lr": 10**(-6)},
    {"params": layer3_weights, "lr": 10**(-5)},
    {"params": layer4_weights, "lr": 10**(-4)},
    {"params": fc_weights, "lr": 10**(-3)},
    {"params": layer2_bias, "lr": 10**(-6), "weight_decay": 0},
    {"params": layer3_bias, "lr": 10**(-5), "weight_decay": 0},
    {"params": layer4_bias, "lr": 10**(-4), "weight_decay": 0},
    {"params": fc_bias, "lr": 10**(-3), "weight_decay": 0}
], weight_decay=10**(-4))

"""# Cosine learning rate decay"""

scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*2)
criterion = nn.CrossEntropyLoss().to(device)

"""# Training of the model"""

loss_ = []
accuracy_ = []

for epoch in range(1, epochs+1):
    model.train()

    progress_bar = tqdm(train_loader,
                        leave=True,
                        unit="batch")

    mean_loss = 0
    m = 0

    for images, labels in progress_bar:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()

        optimizer.step()
        scheduler.step()

        value = loss.item()
        m += 1
        mean_loss = 1/m * value + (1 - 1/m) * mean_loss

        text = f"[train] Epoch: [{epoch}/{epochs}] <Loss>: {mean_loss:.2f}|"
        progress_bar.set_description(text)

    model.eval()
    total = 0
    correct = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)

            predicted = torch.argmax(outputs, dim=1)
            labels = torch.argmax(labels, dim=1)

            total += labels.shape[0]
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f"Accuracy: {accuracy}")

    loss_.append(mean_loss)
    accuracy_.append(accuracy)

plt.plot(range(1, epochs+1), loss_);
plt.title("Loss");
plt.xticks(range(1, epochs+1))
plt.xlabel("Epochs");

plt.plot(range(1, epochs+1), accuracy_);
plt.title("Accuracy");
plt.xticks(range(1, epochs+1))
plt.xlabel("Epochs");

